{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Topic modeling - LDA model on Corona virus text data with Apache Spark and Spark NLP"
      ],
      "metadata": {
        "id": "WUpuYyo9Y8PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Install Java, PySpark, and Spark NLP"
      ],
      "metadata": {
        "id": "M2rys33SZakg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==3.5.3 #Updated\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==5.3.2 #Updated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XjpNAd4ZEJ_",
        "outputId": "aae069eb-61b2-484e-81c7-35e7650733d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "openjdk version \"1.8.0_432\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_432-8u432-ga~us1-0ubuntu2~22.04-ga)\n",
            "OpenJDK 64-Bit Server VM (build 25.432-bga, mixed mode)\n",
            "Collecting pyspark==3.5.3\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/py4j/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting py4j==0.10.9.7 (from pyspark==3.5.3)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=8bab2f3458171e0dc43bf1b66a70faf7a3f8bfb040b1d3592867c630a5b314bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
            "Collecting spark-nlp==5.3.2\n",
            "  Downloading spark_nlp-5.3.2-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spark_nlp-5.3.2-py2.py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-5.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import the relevant package"
      ],
      "metadata": {
        "id": "L7MfhT48Zf24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "! python -V\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh9yTr56ZKk4",
        "outputId": "801cae06-8ae7-40e1-d490-fc5f7effe1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Spark NLP version:  5.3.2\n",
            "Apache Spark version:  3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LDA model on Corona virus text data\n"
      ],
      "metadata": {
        "id": "edkKSNpazeAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To find topics in the coronavirus tweets dataset:\n",
        "\n",
        "\n",
        "Also, tried\n",
        "different values of k and maxIter to see which combination best suits the data.\n",
        "\n",
        "Showed\n",
        "five combinations,  their results, and explained why it’s best.\n",
        "\n",
        "Coronavirus tweets dataset (sample)\n",
        "• Dataset location (Linux filesystem): /home/data/CSC534BDA/datasets/COVID19/\n",
        "i. Dataset filename: coronavirus-text-only-1000.txt\n",
        "ii. Download/copy the file from cscluster\n",
        "1.  use the ‘scp’ Linux command or SFTP clients like\n",
        "CyberDuck or Filezilla.\n",
        "iii. Upload/copy the file to your Colab so you can access the file in your Colab.\n",
        "1.  use Files Explorer in Colab or Google Drive\n",
        "2. For details, https://neptune.ai/blog/google-colab-dealing-with-files\n",
        "\n",
        "• It contains 1000 tweets (text message only) with the common keyword ‘coronavirus’ in\n",
        "their text messages. So ‘coronavirus’ can be considered a stopword and should be\n",
        "removed."
      ],
      "metadata": {
        "id": "-e9L7O5VRMVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Reading file"
      ],
      "metadata": {
        "id": "fUTnXhVKbRQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_location= r'/content/coronavirus-text-only-1000.txt'\n",
        "file_type='lineSep'\n",
        "df=spark.read.option(file_type,\"\\n\")\\\n",
        ".text(file_location)\n",
        "df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgRFAUMuROp4",
        "outputId": "9f82a69c-3fd0-4439-fe4b-576352e21088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show data after reading from file and converted to data frame"
      ],
      "metadata": {
        "id": "g8irgxD6WloL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDSA6oucWl9f",
        "outputId": "706b02fe-5b3f-4c02-9266-ec848c439048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|              text\\r|\n",
            "|Studies look at t...|\n",
            "|\"RT @EricTopol: T...|\n",
            "|\"RT @NPR: Working...|\n",
            "|\"RT @Harvey_Walke...|\n",
            "|RT @CNNEE: La far...|\n",
            "|\"RT @ReutersWorld...|\n",
            "|\"RT @CNN: This Il...|\n",
            "|\"RT @Censelio: Ar...|\n",
            "|RT @jilevin: Trum...|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Pre-processing Pieline using spark-nlp"
      ],
      "metadata": {
        "id": "aEc1vzHQWoKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        ".setInputCol(\"value\") \\\n",
        ".setOutputCol(\"document\") \\\n",
        ".setCleanupMode(\"shrink\")"
      ],
      "metadata": {
        "id": "jaUGVwxkWonW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing data"
      ],
      "metadata": {
        "id": "iNbPFATcWyXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "bvJu-06HWyrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing data"
      ],
      "metadata": {
        "id": "xk7tXYaEW08N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"normalized\")"
      ],
      "metadata": {
        "id": "-36voPQxW1VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords cleaning"
      ],
      "metadata": {
        "id": "j-cHNiv_W6jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        ".setInputCols(\"normalized\")\\\n",
        ".setOutputCol(\"cleanTokens\")\\\n",
        ".setStopWords([\"coronavirus\"])\\\n",
        ".setCaseSensitive(False)"
      ],
      "metadata": {
        "id": "tSYirBHbW63l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming data"
      ],
      "metadata": {
        "id": "EtFiIj6qW9Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer() \\\n",
        ".setInputCols([\"cleanTokens\"]) \\\n",
        ".setOutputCol(\"stem\")"
      ],
      "metadata": {
        "id": "JZ8YULD4W9aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finishing the data"
      ],
      "metadata": {
        "id": "nKxSYnvPW_2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher() \\\n",
        ".setInputCols([\"stem\"]) \\\n",
        ".setOutputCols([\"tokens\"]) \\\n",
        ".setOutputAsArray(True) \\\n",
        ".setCleanAnnotations(False)"
      ],
      "metadata": {
        "id": "fG_TeT3KXALv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline = Pipeline(\n",
        "stages=[document_assembler,\n",
        "tokenizer,\n",
        "normalizer,\n",
        "stopwords_cleaner,\n",
        "stemmer,\n",
        "finisher])\n"
      ],
      "metadata": {
        "id": "Sat4ArAZXEQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train and apply ML pipeline"
      ],
      "metadata": {
        "id": "wOctGqHlffhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model = nlp_pipeline.fit(df)\n",
        "processed_df = nlp_model.transform(df)\n",
        "tokens_df = processed_df.select('tokens').limit(10000)\n",
        "tokens_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdaDzq6IXNBW",
        "outputId": "89364231-193f-4e93-cd8b-812708b90d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              tokens|\n",
            "+--------------------+\n",
            "|              [text]|\n",
            "|[studi, look, at,...|\n",
            "|[rt, erictopol, t...|\n",
            "|[rt, npr, work, m...|\n",
            "|[rt, harveywalk, ...|\n",
            "|[rt, cnnee, la, f...|\n",
            "|[rt, reutersworld...|\n",
            "|[rt, cnn, thi, il...|\n",
            "|[rt, censelio, ar...|\n",
            "|[rt, jilevin, tru...|\n",
            "|[rt, propublica, ...|\n",
            "|[nsw, to, close, ...|\n",
            "|[rt, aslavitt, tr...|\n",
            "|[rt, claytravi, d...|\n",
            "|[rt, jamesgunn, i...|\n",
            "|[rt, natashafatah...|\n",
            "|[rt, crissl, yäôa...|\n",
            "|[rt, censelio, ar...|\n",
            "|[rt, villarruelcl...|\n",
            "|[rt, jaxalemani, ...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering"
      ],
      "metadata": {
        "id": "UoszdWPjXQ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\",\n",
        "vocabSize=500, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(tokens_df)"
      ],
      "metadata": {
        "id": "Cu17L-omXRMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the LDA Model\n",
        "Tried with paramters: num_topics=3, maxIterations=10"
      ],
      "metadata": {
        "id": "rRZ0Mkh2c7-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbkLPUQGXlTs",
        "outputId": "38cddeb1-bc06-4c24-c3ee-4cf2342a8a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -72147.6722530659\n",
            "The upper bound on perplexity: 5.380942142979259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize the topics"
      ],
      "metadata": {
        "id": "AqETy15yXt1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        "  .map(lambda row: row['termIndices'])\\\n",
        "  .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
        "  .collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "  print(\"topic: {}\".format(idx))\n",
        "  print(\"*\"*25)\n",
        "  for word in topic:\n",
        "    print(word)\n",
        "print(\"*\"*25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FO2dceuXuNL",
        "outputId": "3b5f711a-5684-4672-c34d-4395792775a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "de\n",
            "rt\n",
            "la\n",
            "el\n",
            "a\n",
            "en\n",
            "que\n",
            "y\n",
            "lo\n",
            "no\n",
            "topic: 1\n",
            "*************************\n",
            "rt\n",
            "the\n",
            "and\n",
            "in\n",
            "to\n",
            "a\n",
            "of\n",
            "florida\n",
            "their\n",
            "white\n",
            "topic: 2\n",
            "*************************\n",
            "the\n",
            "rt\n",
            "a\n",
            "to\n",
            "in\n",
            "of\n",
            "i\n",
            "trump\n",
            "and\n",
            "she\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tried with 5 more combinations in addition to the above combination as shown below:"
      ],
      "metadata": {
        "id": "vy-8hpIoX8mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) number  of topics= 5, max Iterations= 10"
      ],
      "metadata": {
        "id": "n6jzw472drjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 5\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjoQpcwiX87l",
        "outputId": "77bf20c1-c2f3-46db-bce8-656fefa7f112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -72018.89426431859\n",
            "The upper bound on perplexity: 5.37133757937937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) number  of topics= 7, max Iterations= 20"
      ],
      "metadata": {
        "id": "GZKfl5e5YIbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 7\n",
        "lda= LDA(k=num_topics, maxIter=20)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yu2MtX7YIrH",
        "outputId": "84cf878b-380e-4347-bd33-e9ece7c6588e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -71910.05255982341\n",
            "The upper bound on perplexity: 5.363219910488023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) number  of topics= 11, max Iterations= 23"
      ],
      "metadata": {
        "id": "gk58Ou7TYOP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 11\n",
        "lda= LDA(k=num_topics, maxIter=23)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ9WVzFhYOnF",
        "outputId": "7b254b3b-4ec0-43a8-a19e-deb89d1d6805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -71951.9920213258\n",
            "The upper bound on perplexity: 5.3663478536191676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) number  of topics= 15, max Iterations= 25"
      ],
      "metadata": {
        "id": "9L1EC1DcYYd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 15\n",
        "lda= LDA(k=num_topics, maxIter=25)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff242bFSYZf3",
        "outputId": "0e12e029-9d79-4cc8-bd87-f5f33280f441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -74662.37843135759\n",
            "The upper bound on perplexity: 5.568494811407935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) number  of topics= 18, max Iterations= 28"
      ],
      "metadata": {
        "id": "0sKvreCTYp46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 18\n",
        "lda= LDA(k=num_topics, maxIter=28)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qmRu-ivYqPd",
        "outputId": "0a667b2c-79f4-4d30-faff-dc146748b06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -77558.97745058485\n",
            "The upper bound on perplexity: 5.784529941123571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_topics =5 ,maxIter=10\n",
        "\n",
        "gave better performance out of all the combinations tried above and resulted in higher loglikelihood and lower perplexity values of -72018.89426431859\n",
        "& 5.37133757937937 respectively."
      ],
      "metadata": {
        "id": "Vk2bD2Y6Yxbs"
      }
    }
  ]
}
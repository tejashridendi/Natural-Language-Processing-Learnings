{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Topic modeling with Apache Spark and Spark NLP - LDA model on abc news data"
      ],
      "metadata": {
        "id": "WUpuYyo9Y8PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Java, PySpark, and Spark NLP"
      ],
      "metadata": {
        "id": "M2rys33SZakg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==3.5.3 #Updated\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==5.3.2 #Updated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "4XjpNAd4ZEJ_",
        "outputId": "43bdba46-75b1-42e1-9b37-483f7919f0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "openjdk version \"1.8.0_432\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_432-8u432-ga~us1-0ubuntu2~22.04-ga)\n",
            "OpenJDK 64-Bit Server VM (build 25.432-bga, mixed mode)\n",
            "Collecting pyspark==3.5.3\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.5.3)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=5b6801dc588ec2967b85068aa440a79b23dbe5c485caa8b99b341e4ec89a4127\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              },
              "id": "aa4f42e587c744cfb9993609632139c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark-nlp==5.3.2\n",
            "  Downloading spark_nlp-5.3.2-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spark_nlp-5.3.2-py2.py3-none-any.whl (564 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/565.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m542.7/565.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-5.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the relevant package"
      ],
      "metadata": {
        "id": "L7MfhT48Zf24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "! python -V\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh9yTr56ZKk4",
        "outputId": "c4ebc83c-7bf4-4781-e824-4326c068e23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Spark NLP version:  5.3.2\n",
            "Apache Spark version:  3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LDA model on abc news data"
      ],
      "metadata": {
        "id": "tcsGSR_JzW1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the news data"
      ],
      "metadata": {
        "id": "tS8IzcyCZhrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "download_path = \"./abcnews-date-text.csv\"\n",
        "if not Path(download_path).is_file():\n",
        "  print(\"File Not found will downloading it!\")\n",
        "  url = \"https://github.com/ravishchawla/topic_modeling/raw/master/data/abcnews-date-text.csv\"\n",
        "  urllib.request.urlretrieve(url, download_path)\n",
        "else:\n",
        "  print(\"File already present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de0UBxuHZPuG",
        "outputId": "616889f5-67f0-4fb1-c9d0-f2a88b8d9329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Not found will downloading it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data"
      ],
      "metadata": {
        "id": "fKU-nNdlZld4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you are reading file from local storage\n",
        "file_location = r'./abcnews-date-text.csv'\n",
        "# if you are reading file from hdfs\n",
        "# file_location = r'hdfs:\\\\\\user\\path\\to\\abcnews_date_txt.csv'\n",
        "file_type = \"csv\"\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "# Verify the count\n",
        "df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRa6K3JoZUUB",
        "outputId": "66fe882a-2ea6-4d50-e692-4b511ab804d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1041793"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L8tbQvrZwBU",
        "outputId": "5b7db318-a9a7-4002-9481-710341bced80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|       headline_text|\n",
            "+------------+--------------------+\n",
            "|    20030219|aba decides again...|\n",
            "|    20030219|act fire witnesse...|\n",
            "|    20030219|a g calls for inf...|\n",
            "|    20030219|air nz staff in a...|\n",
            "|    20030219|air nz strike to ...|\n",
            "|    20030219|ambitious olsson ...|\n",
            "|    20030219|antic delighted w...|\n",
            "|    20030219|aussie qualifier ...|\n",
            "|    20030219|aust addresses un...|\n",
            "|    20030219|australia is lock...|\n",
            "+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing Pipeline using Spark NLP"
      ],
      "metadata": {
        "id": "sBz-0UgsZsd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Document Assembling\n",
        "\n",
        "Spark NLP requires the input DataFrame or column to be converted to a document."
      ],
      "metadata": {
        "id": "rL6I_9deZ8t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        ".setInputCol(\"headline_text\") \\\n",
        ".setOutputCol(\"document\") \\\n",
        ".setCleanupMode(\"shrink\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-7gSf8zjZ4yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizing\n",
        "\n",
        "Split sentence to tokens(arraay)"
      ],
      "metadata": {
        "id": "D_fhFj86aU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n"
      ],
      "metadata": {
        "id": "BKspFZZjaFjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing\n",
        "\n",
        "Clean unwanted characters and garbage"
      ],
      "metadata": {
        "id": "yS__S35OaZ7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"normalized\")\n"
      ],
      "metadata": {
        "id": "pFVH9JAHaLRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords removal\n",
        "\n",
        "Remove stopwords"
      ],
      "metadata": {
        "id": "v23b7p1vaeCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        ".setInputCols(\"normalized\")\\\n",
        ".setOutputCol(\"cleanTokens\")\\\n",
        ".setCaseSensitive(False)\n"
      ],
      "metadata": {
        "id": "d67ZRebUaNbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming\n",
        "\n",
        "Stem the words to bring them to the root form."
      ],
      "metadata": {
        "id": "p0wKX2WjajQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer() \\\n",
        ".setInputCols([\"cleanTokens\"]) \\\n",
        ".setOutputCol(\"stem\")"
      ],
      "metadata": {
        "id": "rc0Ud2VHaPzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finishing\n",
        "\n",
        "The finisher is the most important annotator. Spark NLP adds structure when we convert each\n",
        "row in the DataFrame to a document. Finisher helps us to bring back the expected structure and\n",
        "an array of tokens"
      ],
      "metadata": {
        "id": "559ReeAMapZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher() \\\n",
        ".setInputCols([\"stem\"]) \\\n",
        ".setOutputCols([\"tokens\"]) \\\n",
        ".setOutputAsArray(True) \\\n",
        ".setCleanAnnotations(False)\n"
      ],
      "metadata": {
        "id": "s7lt_2rFaq8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the ML Pipeline\n",
        "\n",
        "We build an ML pipeline so that each phase can be executed in sequence. This pipeline can also\n",
        "be used to test the model"
      ],
      "metadata": {
        "id": "72WNMMX1a0hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pipeline = Pipeline(\n",
        "stages=[document_assembler,\n",
        "tokenizer,\n",
        "normalizer,\n",
        "stopwords_cleaner,\n",
        "stemmer,\n",
        "finisher])\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "processed_df = nlp_model.transform(df)\n",
        "tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
        "tokens_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0EAcoblaxf0",
        "outputId": "c4f1a8ab-5b00-4302-af19-314f80bfd61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|              tokens|\n",
            "+------------+--------------------+\n",
            "|    20030219|[aba, decid, comm...|\n",
            "|    20030219|[act, fire, wit, ...|\n",
            "|    20030219|[g, call, infrast...|\n",
            "|    20030219|[air, nz, staff, ...|\n",
            "|    20030219|[air, nz, strike,...|\n",
            "|    20030219|[ambiti, olsson, ...|\n",
            "|    20030219|[antic, delight, ...|\n",
            "|    20030219|[aussi, qualifi, ...|\n",
            "|    20030219|[aust, address, u...|\n",
            "|    20030219|[australia, lock,...|\n",
            "|    20030219|[australia, contr...|\n",
            "|    20030219|[barca, take, rec...|\n",
            "|    20030219|[bathhous, plan, ...|\n",
            "|    20030219|[big, hope, launc...|\n",
            "|    20030219|[big, plan, boost...|\n",
            "|    20030219|[blizzard, buri, ...|\n",
            "|    20030219|[brigadi, dismiss...|\n",
            "|    20030219|[british, combat,...|\n",
            "|    20030219|[bryant, lead, la...|\n",
            "|    20030219|[bushfir, victim,...|\n",
            "+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Apply the ML Pipeline\n",
        "\n",
        "Train and apply the pipeline to transform the dataset. Spark NLP pipeline creates intermediary\n",
        "columns that we don’t need. So, let’s select the columns that we need"
      ],
      "metadata": {
        "id": "EwcIXPqda5od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model = nlp_pipeline.fit(df)\n",
        "processed_df = nlp_model.transform(df)\n",
        "tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
        "tokens_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTlWvB_ua7H0",
        "outputId": "80f4da45-fa3e-41a1-8cb4-f0997f1e0ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|              tokens|\n",
            "+------------+--------------------+\n",
            "|    20030219|[aba, decid, comm...|\n",
            "|    20030219|[act, fire, wit, ...|\n",
            "|    20030219|[g, call, infrast...|\n",
            "|    20030219|[air, nz, staff, ...|\n",
            "|    20030219|[air, nz, strike,...|\n",
            "|    20030219|[ambiti, olsson, ...|\n",
            "|    20030219|[antic, delight, ...|\n",
            "|    20030219|[aussi, qualifi, ...|\n",
            "|    20030219|[aust, address, u...|\n",
            "|    20030219|[australia, lock,...|\n",
            "|    20030219|[australia, contr...|\n",
            "|    20030219|[barca, take, rec...|\n",
            "|    20030219|[bathhous, plan, ...|\n",
            "|    20030219|[big, hope, launc...|\n",
            "|    20030219|[big, plan, boost...|\n",
            "|    20030219|[blizzard, buri, ...|\n",
            "|    20030219|[brigadi, dismiss...|\n",
            "|    20030219|[british, combat,...|\n",
            "|    20030219|[bryant, lead, la...|\n",
            "|    20030219|[bushfir, victim,...|\n",
            "+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n",
        "\n",
        "We will use Spark MLlib’s CountVectorizer to generate features from textual data. Latent\n",
        "Dirichlet Allocation requires a data-specific vocabulary to perform topic modeling."
      ],
      "metadata": {
        "id": "keAwPeYVa917"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\",\n",
        "vocabSize=500, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(tokens_df)"
      ],
      "metadata": {
        "id": "UOhdvb9Wa2T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the LDA Model\n",
        "\n",
        "The LDA model requires a minimum of 2 hyperparameters: k (number of topics) and maxIter\n",
        "(number of iterations). Try different values of k and maxIter to see which combination best suits\n",
        "your data"
      ],
      "metadata": {
        "id": "PIGrI3_RbF4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJLtBS_ibJZL",
        "outputId": "1577856c-a499-4eb4-b864-f87b61d29476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -179061.72753969542\n",
            "The upper bound on perplexity: 6.311212728735916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the topics\n",
        "\n",
        "After completing the training, we can view the words that represent each topic using the\n",
        "following code"
      ],
      "metadata": {
        "id": "6vLHenyybMOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        "  .map(lambda row: row['termIndices'])\\\n",
        "  .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
        "  .collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "  print(\"topic: {}\".format(idx))\n",
        "  print(\"*\"*25)\n",
        "  for word in topic:\n",
        "    print(word)\n",
        "print(\"*\"*25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2CUSKFTbRFC",
        "outputId": "9d492532-42d9-494c-f282-f49e09754b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "man\n",
            "new\n",
            "plan\n",
            "war\n",
            "take\n",
            "charg\n",
            "court\n",
            "face\n",
            "australia\n",
            "mai\n",
            "topic: 1\n",
            "*************************\n",
            "iraq\n",
            "win\n",
            "fire\n",
            "council\n",
            "water\n",
            "back\n",
            "boost\n",
            "call\n",
            "fund\n",
            "mp\n",
            "topic: 2\n",
            "*************************\n",
            "u\n",
            "polic\n",
            "war\n",
            "iraqi\n",
            "iraq\n",
            "kill\n",
            "baghdad\n",
            "claim\n",
            "world\n",
            "warn\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Trying  different values of k and maxIter to see which combination best suits your data in\n",
        "with atleast five combinations, show their results, and explained why it’s best.\n"
      ],
      "metadata": {
        "id": "DDFzSznnbTcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_topics = 5  ,maxIter=10"
      ],
      "metadata": {
        "id": "OdeYD4E2Ptpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 5\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3qjEHdgMhRp",
        "outputId": "58900e2f-0fd3-4e0e-d409-4c37cebeb72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -183895.3794197562\n",
            "The upper bound on perplexity: 6.481579706039624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_topics = 7  ,maxIter=20"
      ],
      "metadata": {
        "id": "ERL-wpBQNHIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 7\n",
        "lda= LDA(k=num_topics, maxIter=20)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH6lWyt8NHeN",
        "outputId": "d3a3c4cf-2958-4782-bbd0-544c5e1595ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -183902.50900390453\n",
            "The upper bound on perplexity: 6.4818309954851445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_topics = 11  ,maxIter=23"
      ],
      "metadata": {
        "id": "5eWSopQ0Ndsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 11\n",
        "lda= LDA(k=num_topics, maxIter=23)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yKvQAfhNeaQ",
        "outputId": "717dbcbb-2949-48c2-f1b6-5da9a1e6c8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -189365.2321814272\n",
            "The upper bound on perplexity: 6.674370230559256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_topics = 15  ,maxIter=25"
      ],
      "metadata": {
        "id": "QRqSm1AFOHc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 15\n",
        "lda= LDA(k=num_topics, maxIter=25)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wcLmlZdOiZS",
        "outputId": "bc9b74d8-5d8c-43e1-ab99-72dd2659badd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -194546.90333226265\n",
            "The upper bound on perplexity: 6.857003501066638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_topics = 18  ,maxIter=28"
      ],
      "metadata": {
        "id": "4yVD7zh9OUq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 18\n",
        "lda= LDA(k=num_topics, maxIter=28)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "popkdAEEOmoE",
        "outputId": "3c3ae38e-5313-4a83-cdd2-ccb42ca5b84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -198008.73156073922\n",
            "The upper bound on perplexity: 6.979019158351164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher loglikelihood and lower perplexity represents better model performance.**\n",
        "\n",
        "Hence, num_topics = 5, maxIter=10 gave better results with the lower bound on the log likelihood of the entire corpus of -183895.3794197562\n",
        "the upper bound on perplexity 6.481579706039624"
      ],
      "metadata": {
        "id": "LrpVDM4ROr79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================THE END======================================="
      ],
      "metadata": {
        "id": "DAJJyVobCNPo"
      }
    }
  ]
}